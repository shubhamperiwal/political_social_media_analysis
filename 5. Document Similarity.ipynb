{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will enter a query and print out the most similar tweets from the 3 presidential candidates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smart\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import similarities\n",
    "from gensim import models\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = pd.read_csv(\"data/DonaldTrumpClean.csv\")\n",
    "obama = pd.read_csv(\"data/BarackObamaClean.csv\")\n",
    "clinton = pd.read_csv(\"data/HillaryClintonClean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean the text further to remove anything that is not useful in calculating similarity\n",
    "def clean(docs):\n",
    "    #Convert all to lower case\n",
    "    docs_lower = [[w.lower() for w in doc] for doc in docs]\n",
    "    \n",
    "    #Remove all non-word characters\n",
    "    docs_regex= [[w for w in doc if re.search('^[a-z]+$',w)] for doc in docs_lower]\n",
    "    \n",
    "    #Remove all stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    docs_stop = [[w for w in doc if w not in stop_words] for doc in docs_regex]\n",
    "    \n",
    "    #Stem all the worsd\n",
    "    docs_stem = [[stemmer.stem(w) for w in doc] for doc in docs_stop]\n",
    "    return docs_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map each word to an ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to map each unique word in the vocabulary of this corpus to an ID or index first. \n",
    "#These mappings from words to IDs are represented by a class called Dictionary in Gensim.\n",
    "\n",
    "text_trump = trump['text'].map(lambda x: x.split())\n",
    "docs_trump = clean(text_trump)\n",
    "dictionaryTrump = corpora.Dictionary(docs_trump)\n",
    "dictionaryTrump2 = corpora.Dictionary(text_trump)\n",
    "\n",
    "text_obama = obama['text'].map(lambda x: x.split())\n",
    "docs_obama = clean(text_obama)\n",
    "dictionaryObama = corpora.Dictionary(docs_obama)\n",
    "\n",
    "text_clinton = clinton['text'].map(lambda x: x.split())\n",
    "docs_clinton = clean(text_clinton)\n",
    "dictionaryClinton = corpora.Dictionary(docs_clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(3375 unique tokens: ['let', 'bright', 'everyon', 'go', 'know']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionaryClinton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the data where each token is mapped to an ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#We can use dictionary.token2id to obtain a dict object which contains all the mappings.\n",
    "token_to_id_trump = dictionaryTrump.token2id\n",
    "token_to_id_obama = dictionaryObama.token2id\n",
    "token_to_id_clinton = dictionaryClinton.token2id\n",
    "print(type(token_to_id_obama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5803 unique tokens: ['realli', 'badli', 'drop', 'establish', 'let']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionaryTrump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert documents to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we use the function doc2bow to convert our documents to vectors . Here bow stands for bag of words\n",
    "vec_trump = [dictionaryTrump.doc2bow(doc) for doc in docs_trump]\n",
    "vec_obama = [dictionaryObama.doc2bow(doc) for doc in docs_obama]\n",
    "vec_clinton = [dictionaryClinton.doc2bow(doc) for doc in docs_clinton]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_trump = similarities.SparseMatrixSimilarity(vec_trump, len(dictionaryTrump.items()))\n",
    "index_obama = similarities.SparseMatrixSimilarity(vec_obama, len(dictionaryObama.items()))\n",
    "index_clinton = similarities.SparseMatrixSimilarity(vec_clinton, len(dictionaryClinton.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TFIDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_trump = models.TfidfModel(vec_trump)\n",
    "vecs_with_tfidf_trump = []\n",
    "for doc in vec_trump:\n",
    "    vecs_with_tfidf_trump.append(tfidf_trump[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_obama = models.TfidfModel(vec_obama)\n",
    "vecs_with_tfidf_obama = []\n",
    "for doc in vec_obama:\n",
    "    vecs_with_tfidf_obama.append(tfidf_obama[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_clinton = models.TfidfModel(vec_clinton)\n",
    "vecs_with_tfidf_clinton = []\n",
    "for doc in vec_clinton:\n",
    "    vecs_with_tfidf_clinton.append(tfidf_clinton[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the most similar tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter query here\n",
    "query = 'Mexico'\n",
    "query_stem = [stemmer.stem(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilarTweet(dictionary, tfidf_model, index, df):\n",
    "    query_vec = dictionary.doc2bow(query_stem)\n",
    "    query_vec_tfidf = tfidf_model[query_vec]\n",
    "    q_similarity = index[query_vec_tfidf]\n",
    "    q_sim_sorted = sorted(enumerate(q_similarity), key = lambda item: -item[1])\n",
    "    similar_tweet = df['text'][q_sim_sorted[0][0]]\n",
    "    return similar_tweet.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_trump = findSimilarTweet(dictionaryTrump, tfidf_trump, index_trump, trump)\n",
    "similar_obama = findSimilarTweet(dictionaryObama, tfidf_obama, index_obama, obama)\n",
    "similar_clinton = findSimilarTweet(dictionaryClinton, tfidf_clinton, index_clinton, clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Mexico\n",
      "\n",
      "Trump's similar tweet:  Mexico will pay for the wall!\n",
      "\n",
      "Obama's similar tweet:  Denying climate change is dangerous. Join  supporters in standing up for bold action now http//ofa.bo/2dZNTRx #ActOnClimate\n",
      "\n",
      "Clinton's similar tweet:  A wall that Mexico will pay for a bad idea from an even worse negotiator.\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: \", query)\n",
    "print(\"\\nTrump's similar tweet: \", similar_trump)\n",
    "print(\"\\nObama's similar tweet: \", similar_obama)\n",
    "print(\"\\nClinton's similar tweet: \", similar_clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
